
import functools
import pickle
import os

class PersistentLRUCache:
    def __init__(self, maxsize=128, cache_file='cache.pkl'):
        self.cache_file = cache_file
        self.lru_cache = functools.lru_cache(maxsize=maxsize)(self._call_function)
        self.load_cache()

    def __call__(self, func):
        self.func = func
        return self.lru_cache

    def _call_function(self, *args, **kwargs):
        """Call the actual function with the cached behavior."""
        return self.func(*args, **kwargs)

    def save_cache(self):
        """Save the cache to a file."""
        with open(self.cache_file, 'wb') as f:
            # Export cache contents (internal cache and corresponding function arguments)
            cache_data = {
                'cache': self.lru_cache.cache_info(),
                'lru_cache': self.lru_cache.cache_clear()  # Save current state
            }
            pickle.dump(cache_data, f)

    def load_cache(self):
        """Load the cache from a file."""
        if os.path.exists(self.cache_file):
            with open(self.cache_file, 'rb') as f:
                cache_data = pickle.load(f)
                # Restore the cache data into the lru_cache (if possible)
                self.lru_cache.cache_clear()

    def cache_info(self):
        """Return cache statistics like hits and misses."""
        return self.lru_cache.cache_info()

    def clear_cache(self):
        """Clear the in-memory cache."""
        self.lru_cache.cache_clear()
        if os.path.exists(self.cache_file):
            os.remove(self.cache_file)


# Example usage of PersistentLRUCache

@PersistentLRUCache(maxsize=100, cache_file='my_lru_cache.pkl')
def expensive_function(x):
    print(f"Calculating for {x}...")
    return x * x

# Example usage
print(expensive_function(10))  # First time calculates and caches
print(expensive_function(10))  # Second time uses the cache

# Save the current cache state to disk
expensive_function.save_cache()

# Clear the cache (for example, restart of application)
expensive_function.clear_cache()

# Load the cache back from the file
expensive_function.load_cache()
print(expensive_function(10))  # Should load from the previously saved cache




import threading
import time
import sys

def periodic_function():
    try:
        while True:
            # Your periodic task here
            print("Running periodic task...")
            time.sleep(2)  # Simulate periodic work
            # Raise an exception to simulate failure
            raise Exception("Something went wrong in the daemon thread!")
    except Exception as e:
        print(f"Exception caught in daemon thread: {e}")
        # Allow the thread to terminate after catching the exception

def main():
    thread = threading.Thread(target=periodic_function, daemon=True)
    thread.start()

    try:
        while thread.is_alive():
            time.sleep(1)  # Main thread can do other work here

    except KeyboardInterrupt:
        print("Main program interrupted by user.")

    finally:
        print("Shutting down the program.")
        if not thread.is_alive():
            sys.exit(1)  # Exit the program with an error code

if __name__ == "__main__":
    main()





When interviewing a candidate for a Python role, asking questions that test both their understanding of language fundamentals and their ability to solve tricky problems is essential. Here are some challenging and thought-provoking Python interview questions:

1. Mutable Default Arguments

	•	Question: What happens when you use a mutable default argument (like a list or dictionary) in a function? How would you avoid issues with it?
	•	What it tests: Understanding of default argument behavior and potential bugs related to mutability.
	•	Answer: The default argument is evaluated only once, so if it’s modified (like appending to a list), the changes persist across function calls. To avoid this, use None as a default value and initialize inside the function.

2. Variable Scope and Closures

	•	Question: What is a closure in Python? Can you give an example?
	•	What it tests: Knowledge of how functions interact with their enclosing scopes and understanding of closures.
	•	Answer: A closure is a function that captures the local variables from its enclosing scope. Example:

def outer(x):
    def inner(y):
        return x + y
    return inner



3. The is vs == Operator

	•	Question: What is the difference between the is and == operators in Python?
	•	What it tests: Understanding of object identity (is) vs. equality (==).
	•	Answer: is checks for object identity (whether two references point to the same object in memory), while == checks for value equality (whether the values of the objects are the same).

4. Function Decorators

	•	Question: What is a decorator in Python? How would you write a decorator that prints the execution time of a function?
	•	What it tests: Understanding of higher-order functions and decorators.
	•	Answer: A decorator is a function that takes another function and extends its behavior without explicitly modifying it. Example of a timing decorator:

import time

def timer(func):
    def wrapper(*args, **kwargs):
        start_time = time.time()
        result = func(*args, **kwargs)
        end_time = time.time()
        print(f"Execution time: {end_time - start_time} seconds")
        return result
    return wrapper



5. Unpacking

	•	Question: How does unpacking work in Python, and what is extended unpacking (*)?
	•	What it tests: Understanding of tuple/list unpacking and advanced unpacking features.
	•	Answer: Unpacking assigns the values of a sequence to multiple variables. Extended unpacking (*) allows for capturing the remaining elements:

a, *b, c = [1, 2, 3, 4, 5]  # a = 1, b = [2, 3, 4], c = 5



6. GIL (Global Interpreter Lock)

	•	Question: What is the GIL in Python, and how does it affect multi-threaded programs?
	•	What it tests: Knowledge of Python’s concurrency model and limitations of the GIL in multi-threaded applications.
	•	Answer: The Global Interpreter Lock (GIL) is a mutex that protects access to Python objects, preventing multiple native threads from executing Python bytecodes at once. This means multi-threaded programs don’t benefit from parallel execution in CPU-bound tasks.

7. @staticmethod vs @classmethod

	•	Question: What is the difference between a @staticmethod and a @classmethod?
	•	What it tests: Understanding of object-oriented programming concepts in Python.
	•	Answer: A @staticmethod is a method that does not receive the instance (self) or the class (cls) as the first argument, while a @classmethod receives the class (cls) as the first argument, allowing it to modify class-level data.

8. Python Memory Management

	•	Question: How does Python manage memory? What are reference counting and garbage collection?
	•	What it tests: Understanding of Python’s memory management and garbage collection mechanism.
	•	Answer: Python uses reference counting to keep track of the number of references to each object. When the reference count drops to zero, the object is deallocated. Python also has a garbage collector that handles circular references.

9. List Comprehensions and Generator Expressions

	•	Question: What’s the difference between a list comprehension and a generator expression?
	•	What it tests: Understanding of memory efficiency and when to use one over the other.
	•	Answer: List comprehensions create a full list in memory, while generator expressions return an iterator that computes the items lazily, one at a time, making them more memory efficient.

10. Metaclasses

	•	Question: What is a metaclass in Python? How can you use a metaclass to change class behavior?
	•	What it tests: Advanced knowledge of Python’s class creation process.
	•	Answer: A metaclass is a class of a class, controlling the creation of classes. You can define custom behavior in the metaclass to influence how classes are created.

11. Coroutines and async/await

	•	Question: How do Python’s coroutines work? What is the purpose of async and await keywords?
	•	What it tests: Knowledge of asynchronous programming in Python.
	•	Answer: Coroutines are special functions that can pause and resume their execution using await. The async keyword is used to define a coroutine, and await is used to wait for asynchronous operations.

12. Python’s with Statement

	•	Question: What is the with statement, and how does it work internally?
	•	What it tests: Understanding of context managers and resource management.
	•	Answer: The with statement is used for resource management, such as opening and closing files. Internally, it uses context managers that implement the __enter__() and __exit__() methods to handle setup and cleanup.

13. Monkey Patching

	•	Question: What is monkey patching in Python? Is it a good practice?
	•	What it tests: Understanding of Python’s dynamic nature and potential pitfalls.
	•	Answer: Monkey patching is the practice of modifying or extending a class or module at runtime. It can be useful in certain situations but is generally discouraged as it can make the code harder to understand and maintain.

14. Comprehensions with Multiple for Loops

	•	Question: How would you write a list comprehension with multiple for loops and conditions?
	•	What it tests: Knowledge of Python’s comprehensions.
	•	Answer: You can nest for loops and include conditions in comprehensions:

result = [x * y for x in range(1, 4) for y in range(1, 3) if x * y > 2]



These types of questions will give you a good sense of the candidate’s depth of knowledge in Python, their understanding of common pitfalls, and their ability to write clean, efficient code.













apiVersion: batch/v1
kind: CronJob
metadata:
  name: scale-deployment-cronjob
spec:
  schedule: "0 10 * * 1-5"  # Every weekday at 10 AM
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: scale-deployment
            image: bitnami/kubectl:latest  # Lightweight kubectl image
            command:
            - /bin/sh
            - -c
            - |
              kubectl scale deployment <your-deployment-name> --replicas=0 -n <your-namespace> && \
              echo "Waiting for pods to terminate..." && \
              sleep 10 && \
              kubectl scale deployment <your-deployment-name> --replicas=1 -n <your-namespace>
          restartPolicy: OnFailure




import json
from datetime import datetime

def transform_json_list(json_list):
    result = [
        {
            'dv01': json.loads(j)['Col0']['0'],
            'maturity': datetime.fromordinal(json.loads(j)['Col0']['1'])
        }
        for j in json_list if j is not None
    ]
    return result




import dash
from dash import dcc, html
from dash_extensions import Clipboard
import dash_table
import pandas as pd

app = dash.Dash(__name__)

# Sample DataFrame
df = pd.DataFrame({
    "Column A": [1, 2, 3],
    "Column B": [4, 5, 6],
    "Column C": ["A", "B", "C"]
})

# Convert DataFrame to HTML table format (for copying)
df_string = df.to_csv(index=False)

app.layout = html.Div([
    # Display the DataFrame as a table
    dash_table.DataTable(
        id='table',
        columns=[{"name": i, "id": i} for i in df.columns],
        data=df.to_dict('records')
    ),

    # Hidden Textarea for copying the DataFrame as plain text (CSV format)
    dcc.Textarea(
        id='table-textarea',
        value=df_string,
        style={"display": "none"}  # Hide the textarea
    ),

    # Copy button
    html.Button("Copy Table to Clipboard", id="copy-button"),

    # Clipboard component to copy the content of the hidden textarea
    Clipboard(target_id="table-textarea"),
])

if __name__ == '__main__':
    app.run_server(debug=True)






from celery import Celery
from datetime import datetime, timedelta

# Initialize Celery
app = Celery('your_app_name')

def check_workers(worker_names):
    # Get the current state of workers
    inspect = app.control.inspect()
    active_workers = inspect.active()

    for worker in worker_names:
        print(f"Checking worker: {worker}")
        
        # Check if worker is active
        if worker not in active_workers:
            print(f"Worker {worker} is down or unresponsive.")
            continue
        
        # Ping the worker
        ping_response = app.control.ping([worker])
        if not ping_response or worker not in ping_response[0]:
            print(f"Worker {worker} did not respond to ping.")
            continue
        
        # Check heartbeat
        stats = inspect.stats()
        heartbeat = stats.get(worker, {}).get('heartbeat')
        if not heartbeat:
            print(f"Worker {worker} has no heartbeat.")
            continue
        
        # Get current time and convert heartbeat to datetime
        heartbeat_time = datetime.fromtimestamp(heartbeat)
        if datetime.now() - heartbeat_time > timedelta(minutes=2):  # Customize threshold if needed
            print(f"Worker {worker} heartbeat is outdated (last seen at {heartbeat_time}).")
            continue
        
        # Check the last task run time
        active_tasks = inspect.active([worker])[worker]
        if active_tasks:
            last_task_time = max(task['time_start'] for task in active_tasks)
            if datetime.now() - datetime.fromtimestamp(last_task_time) > timedelta(minutes=10):
                print(f"Worker {worker} has not run any task for over 10 minutes.")
            else:
                print(f"Worker {worker} is healthy.")
        else:
            print(f"No active tasks on worker {worker}, but it's healthy.")












To use WebSocket in your Dash app with the `dash-extensions` library, you need to replace the interval component and its callback with a WebSocket component that can handle real-time data updates. This way, the data will be pushed to the client as soon as it's available, reducing latency compared to polling with intervals.

Here's a basic example of how to implement this:

### 1. Install Necessary Packages
Make sure you have the necessary packages installed:

```bash
pip install dash dash-extensions
```

### 2. Basic WebSocket Example

```python
import dash
from dash import html, dcc, Output, Input
import dash_extensions as de
import time
import threading

# Sample data update function
def data_generator():
    while True:
        # Simulate some real-time data generation
        yield {"value": time.time()}
        time.sleep(1)  # Adjust frequency as needed

# Create the Dash app
app = dash.Dash(__name__)

# Layout with WebSocket component
app.layout = html.Div([
    de.WebSocket(id="ws", url="ws://127.0.0.1:8000/ws"),  # Replace with your WebSocket server URL
    html.Div(id="live-update-text"),
])

# Callback to update the text with WebSocket data
@app.callback(
    Output("live-update-text", "children"),
    Input("ws", "message")  # Trigger on WebSocket messages
)
def update_text(message):
    data = message["data"]
    return f"Updated value: {data['value']}"

# Run a background thread to simulate WebSocket server
def run_ws_server():
    from flask import Flask, jsonify
    from flask_sockets import Sockets
    from gevent import pywsgi
    from geventwebsocket.handler import WebSocketHandler

    server = Flask(__name__)
    sockets = Sockets(server)

    @sockets.route('/ws')
    def ws(ws):
        generator = data_generator()
        for data in generator:
            if not ws.closed:
                ws.send(jsonify(data).data)
            else:
                break

    # Start the WebSocket server
    server = pywsgi.WSGIServer(("0.0.0.0", 8000), server, handler_class=WebSocketHandler)
    server.serve_forever()

# Run the WebSocket server in a separate thread
thread = threading.Thread(target=run_ws_server)
thread.daemon = True
thread.start()

# Run the Dash app
if __name__ == '__main__':
    app.run_server(debug=True)
```

### Explanation

1. **WebSocket Component**: `de.WebSocket(id="ws", url="ws://127.0.0.1:8000/ws")`
   - This component connects to the WebSocket server at the specified URL. Replace the URL with your WebSocket server's URL.

2. **Callback with WebSocket Input**:
   - The callback is triggered every time a message is received from the WebSocket server (`Input("ws", "message")`).
   - `message["data"]` contains the data sent by the WebSocket server.

3. **Simulated WebSocket Server**:
   - For demonstration purposes, a WebSocket server is created using Flask-Sockets and run in a separate thread.
   - It uses the `data_generator()` function to simulate data updates.

### Customizing for Your Use Case

- **WebSocket Server URL**: Replace `"ws://127.0.0.1:8000/ws"` with your actual WebSocket server URL.
- **Data Format**: Modify the `data_generator()` and callback function as needed to handle your specific data format.

This approach can reduce latency since it pushes data updates to the client as soon as they're available.





NGINX provides various variables (`$values`) that you can use in your configuration to capture and log different aspects of the request and response. Here are some of the common variables available in NGINX:

### Common NGINX Variables

1. **Client Information:**
   - `$remote_addr`: Client IP address.
   - `$remote_port`: Client port.
   - `$remote_user`: User name supplied with the Basic Authentication.

2. **Request Details:**
   - `$request`: Full original request line.
   - `$request_body`: Full request body.
   - `$request_method`: Request method, e.g., `GET` or `POST`.
   - `$request_uri`: Full original request URI (with arguments).

3. **Headers:**
   - `$http_host`: Host name from the request header.
   - `$http_user_agent`: User-Agent header from the client.
   - `$http_referer`: Referer header from the client.
   - `$http_<header>`: Any other HTTP header (replace `<header>` with the header name, e.g., `$http_x_forwarded_for`).

4. **Response Details:**
   - `$status`: Response status code.
   - `$body_bytes_sent`: Number of bytes sent in the response body.
   - `$bytes_sent`: Total number of bytes sent to the client.

5. **Server Information:**
   - `$server_addr`: Server IP address.
   - `$server_port`: Server port.
   - `$server_protocol`: Protocol used, e.g., `HTTP/1.1`.

6. **Connection Information:**
   - `$connection`: Connection serial number.
   - `$connection_requests`: Number of requests made through this connection.

7. **Time and Date:**
   - `$time_local`: Local time in the Common Log Format.
   - `$msec`: Current time in seconds with milliseconds.

### Customizing Logs with Variables

You can use these variables to customize the log format in your NGINX configuration. For example:

```nginx
log_format custom '$remote_addr - $remote_user [$time_local] "$request" '
                  '$status $body_bytes_sent "$http_referer" '
                  '"$http_user_agent" "$http_x_forwarded_for"';
```

### Applying Custom Log Format in Ingress

If you want to use these variables in your Ingress annotations to log client IP and other details, you can specify a custom log format:

```yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: example-ingress
  namespace: your-namespace
  annotations:
    nginx.ingress.kubernetes.io/configuration-snippet: |
      log_format custom '$remote_addr - $http_x_forwarded_for [$time_local] "$request" '
                        '$status $body_bytes_sent "$http_referer" '
                        '"$http_user_agent" "$http_x_forwarded_for"';
    nginx.ingress.kubernetes.io/access-log-format: |
      '$remote_addr - $http_x_forwarded_for [$time_local] "$request" '
      '$status $body_bytes_sent "$http_referer" '
      '"$http_user_agent" "$http_x_forwarded_for"';
spec:
  rules:
  - host: example.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: example-service
            port:
              number: 80
```

### Summary

NGINX provides a rich set of variables that you can use to capture various details about requests, responses, and client/server information. These variables can be used in log formats and configuration snippets to customize how NGINX handles and logs traffic, helping you get the specific details you need for monitoring and debugging.


-- Step 1: Create a sample table
CREATE TABLE sample_times (
    id NUMBER PRIMARY KEY,
    time VARCHAR2(6)
);

-- Step 2: Insert sample data
INSERT INTO sample_times (id, time) VALUES (1, '150000'); -- 15:00:00
INSERT INTO sample_times (id, time) VALUES (2, '160000'); -- 16:00:00
INSERT INTO sample_times (id, time) VALUES (3, '163000'); -- 16:30:00
INSERT INTO sample_times (id, time) VALUES (4, '164500'); -- 16:45:00

-- Step 3: Update times by adding 2 hours, but ensure they do not exceed '170000'
UPDATE sample_times
SET time = (
    SELECT TO_CHAR(
               TRUNC(LEAST((TO_NUMBER(SUBSTR(time, 1, 2)) * 3600 + 
                            TO_NUMBER(SUBSTR(time, 3, 2)) * 60 + 
                            TO_NUMBER(SUBSTR(time, 5, 2)) + 7200), 61200) / 3600, 0), 'FM00') ||
           TO_CHAR(
               TRUNC(MOD(LEAST((TO_NUMBER(SUBSTR(time, 1, 2)) * 3600 + 
                                TO_NUMBER(SUBSTR(time, 3, 2)) * 60 + 
                                TO_NUMBER(SUBSTR(time, 5, 2)) + 7200), 61200), 3600) / 60, 0), 'FM00') ||
           TO_CHAR(
               MOD(LEAST((TO_NUMBER(SUBSTR(time, 1, 2)) * 3600 + 
                          TO_NUMBER(SUBSTR(time, 3, 2)) * 60 + 
                          TO_NUMBER(SUBSTR(time, 5, 2)) + 7200), 61200), 60), 'FM00')
    FROM dual
);

-- Verify the update
SELECT * FROM sample_times;

nested_dict = {
    'a': {
        'b1': {'c1': 1, 'c2': 2, 'c3': 3},
        'b2': {'c4': 4, 'c5': 5, 'c6': 6}
    },
    'd': {
        'e1': {'f1': 7, 'f2': 8, 'f3': 9},
        'e2': {'f4': 10, 'f5': 11, 'f6': 12}
    }
}

nested_dict = {k1: {k2: {list(v3.items())[0][0]: list(v3.items())[0][1]} for k2, v3 in v2.items()} for k1, v2 in nested_dict.items()}

print(nested_dict)


nested_dict = {
    'a': {
        'b1': {'c1': 1, 'c2': 2, 'c3': 3},
        'b2': {'c4': 4, 'c5': 5, 'c6': 6}
    },
    'd': {
        'e1': {'f1': 7, 'f2': 8, 'f3': 9},
        'e2': {'f4': 10, 'f5': 11, 'f6': 12}
    }
}

nested_dict = {k1: {k2: {k3: v3[next(iter(v3))]} for k2, v3 in v2.items()} for k1, v2 in nested_dict.items()}

print(nested_dict)


Sure, I can explain how this process typically works.

### Smartcard Authentication

1. **Smartcard Login**: When you log in to your corporate laptop, you insert your smartcard into a card reader and enter a PIN. The smartcard contains a digital certificate that is used to authenticate you.

2. **Certificate Authentication**: The digital certificate on the smartcard is read by the system and used to authenticate you against the corporate network. This certificate contains your public key, and your private key remains on the card.

### Kerberos Authentication

1. **Initial Authentication Request**: Once your identity is verified through the smartcard, your computer sends an Authentication Service (AS) request to the Kerberos Key Distribution Center (KDC).

2. **Ticket Granting Ticket (TGT)**: The KDC verifies your credentials and issues a Ticket Granting Ticket (TGT), which is encrypted with a key derived from your password (in this case, your smartcard credentials).

3. **Service Ticket Request**: When you try to access an internal website or service, your computer uses the TGT to request a service ticket from the KDC for the specific service.

4. **Service Ticket**: The KDC issues a service ticket, which is encrypted with the service's key.

5. **Service Authentication**: Your computer presents the service ticket to the internal website or service, which decrypts the ticket and verifies your identity.

### Combined Workflow

1. **Smartcard and Kerberos Integration**: Your smartcard login credentials are used to obtain a Kerberos TGT. This TGT is then used to obtain service tickets for accessing internal websites and services without requiring you to re-enter your credentials.

2. **Single Sign-On (SSO)**: The combination of smartcard authentication and Kerberos provides a Single Sign-On (SSO) experience. Once you are logged into your laptop with your smartcard, you can access multiple internal resources without needing to re-authenticate.

### Security Advantages

1. **Strong Authentication**: The use of smartcards provides strong two-factor authentication (something you have - the card, and something you know - the PIN).

2. **Credential Protection**: Smartcards protect your private key and other credentials, making it difficult for attackers to steal them.

3. **Centralized Authentication**: Kerberos allows for centralized management of authentication, making it easier to enforce security policies and manage user access.

This process ensures secure and seamless access to internal resources while minimizing the need for multiple logins.


SELECT
    SUBSTR(email, 1, INSTR(email, '.') - 1) AS Firstname,
    SUBSTR(email, INSTR(email, '.') + 1, INSTR(email, '@') - INSTR(email, '.') - 1) AS Lastname
FROM
    your_table;

def reverse_transform(inverted_dict):
    # Initialize the output dictionary
    output_dict = {}

    # Iterate through the outer dictionary of the inverted structure
    for key3, sub_dict1 in inverted_dict.items():
        for key1, sub_dict2 in sub_dict1.items():
            for key2, value in sub_dict2.items():
                # Ensure key1 is in the outer dictionary of the output
                if key1 not in output_dict:
                    output_dict[key1] = {}
                # Ensure key2 is in the dictionary corresponding to key1
                if key2 not in output_dict[key1]:
                    output_dict[key1][key2] = {}
                # Set the value
                output_dict[key1][key2][key3] = value

    return output_dict

# Example usage:
inverted_dict = {
    'k1': {
        'A': {'a1': 1, 'a2': 3},
        'B': {'b2': 7}
    },
    'k2': {
        'A': {'a1': 2},
        'B': {'b1': 5, 'b2': 8}
    },
    'k3': {
        'A': {'a2': 4},
        'B': {'b1': 6}
    }
}

output_dict = reverse_transform(inverted_dict)
print(output_dict)






def inverse_dict(input_dict):
    # Initialize the output dictionary
    output_dict = {}

    # Iterate through the outer dictionary
    for key1, sub_dict1 in input_dict.items():
        for key2, sub_dict2 in sub_dict1.items():
            for key3, value in sub_dict2.items():
                # Ensure the key3 is in the outer dictionary of the output
                if key3 not in output_dict:
                    output_dict[key3] = {}
                # Ensure key1 is in the dictionary corresponding to key3
                if key1 not in output_dict[key3]:
                    output_dict[key3][key1] = {}
                # Set the value
                output_dict[key3][key1][key2] = value

    return output_dict

# Example usage:
input_dict = {
    'A': {
        'a1': {'k1': 1, 'k2': 2},
        'a2': {'k1': 3, 'k3': 4}
    },
    'B': {
        'b1': {'k2': 5, 'k3': 6},
        'b2': {'k1': 7, 'k2': 8}
    }
}

output_dict = inverse_dict(input_dict)
print(output_dict)


import time
from datetime import datetime, timedelta

def sleep_until_next_day():
    now = datetime.now()
    # Calculate the next midnight
    next_midnight = (now + timedelta(days=1)).replace(hour=0, minute=0, second=0, microsecond=0)
    # Calculate the number of seconds to sleep
    time_to_sleep = (next_midnight - now).total_seconds()
    print(f"Sleeping for {time_to_sleep} seconds until next day")
    time.sleep(time_to_sleep)

# Example usage
if __name__ == "__main__":
    print("Current time:", datetime.now())
    sleep_until_next_day()
    print("Woke up at:", datetime.now())


Using threads is a common approach, but if you want more control over scheduling and better performance, especially if your tasks are I/O-bound rather than CPU-bound, you might consider using the `concurrent.futures` module with `ThreadPoolExecutor` or `asyncio` for asynchronous execution.

Here's how you can use `concurrent.futures.ThreadPoolExecutor`:

```python
import concurrent.futures
import time

def function_1():
    while True:
        # Your code for function 1
        print("Function 1 is running")
        time.sleep(1)

def function_2():
    while True:
        # Your code for function 2
        print("Function 2 is running")
        time.sleep(1)

def run_periodically(executor, func):
    future = executor.submit(func)
    future.add_done_callback(lambda f: run_periodically(executor, func))

# Create a ThreadPoolExecutor
with concurrent.futures.ThreadPoolExecutor(max_workers=2) as executor:
    run_periodically(executor, function_1)
    run_periodically(executor, function_2)

    # Keep the main thread alive to allow periodic execution
    try:
        while True:
            time.sleep(1)
    except KeyboardInterrupt:
        print("Stopped by user")
```

In this example:

1. `run_periodically` submits a function to the executor and re-submits it once it's done, creating a periodic execution loop.
2. `ThreadPoolExecutor` manages the threads for you and allows better control over the number of concurrent threads.

Alternatively, you can use `asyncio` for asynchronous execution:

```python
import asyncio

async def function_1():
    while True:
        # Your code for function 1
        print("Function 1 is running")
        await asyncio.sleep(1)

async def function_2():
    while True:
        # Your code for function 2
        print("Function 2 is running")
        await asyncio.sleep(1)

async def main():
    # Schedule the periodic tasks
    task1 = asyncio.create_task(function_1())
    task2 = asyncio.create_task(function_2())
    
    # Keep the main task alive
    await asyncio.gather(task1, task2)

# Run the asyncio event loop
asyncio.run(main())
```

In this example:

1. `asyncio` runs coroutines (`function_1` and `function_2`) asynchronously.
2. `asyncio.create_task` schedules the coroutines to run concurrently.
3. `asyncio.run(main())` starts the asyncio event loop, executing the main coroutine which gathers the periodic tasks.

Using `asyncio` can be more efficient than threading for I/O-bound tasks since it doesn't require as many system resources and can handle a larger number of concurrent tasks.






### Functional Specifications Document (FSD)

**Project Title**: Data Feed Detection, Onboarding, and Reconciliation Application

**Prepared by**: [Your Name]

**Date**: [Date]

---

#### 1. Introduction

**1.1 Purpose**

This document provides the functional specifications for an application designed to detect new data feeds entering a database, initiate a workflow for onboarding and approval, and perform reconciliation to ensure the accuracy and completeness of data used by production surveillance models. The application aims to streamline the detection and onboarding process, improve data governance, and ensure regulatory compliance.

**1.2 Scope**

The application will:
- Detect new data feeds based on daily load logs.
- Notify database administrators and require their input on the new feeds.
- Notify surveillance business analysts for compliance review and onboarding status.
- Detect persistency tables used by production surveillance models.
- Perform reconciliation to detect mismatches and potential gaps between detected data feeds and persistency tables.

#### 2. Functional Requirements

**2.1 Data Feed Detection**

- The application will scan the database daily to detect any new tables with successfully loaded data. This ensures timely awareness of new data sources.
- Detection will be based on the analysis of load logs stored in the database, which log the status of data loads.

**2.2 Workflow Initiation**

- Upon detection of a new data feed, the application will automatically send an email notification to the database administrators.
- A workflow will be initiated, requiring the database administrators to fill out specific information about the new feed, ensuring that all necessary details are captured for further processing.

**2.3 Administrator Dashboard**

- A dedicated section for database administrators will be available in the dashboard. This provides a centralized location for managing new data feeds.
- Database administrators must provide the following details:
  - Loader name
  - Frequency
  - Data type
  - Upstream source application ID
  - Upstream source application name
  - Persistency table and schema
- Database administrators must also approve whether the source is relevant for regulatory purposes, such as trade, pre-trade, or market data. This ensures that only pertinent data is flagged for regulatory scrutiny.

**2.4 Notification to Surveillance Business Analysts**

- After database administrators complete their input, surveillance business analysts will receive an email notification.
- Analysts will access a dedicated subsection in the dashboard to analyze the new feed. This ensures a systematic review process by the compliance teams.

**2.5 Analyst Review**

- Surveillance business analysts, along with the FOCS and compliance teams, will review the new feed.
- Analysts will determine whether the new feed should be used for surveillance models. This step is crucial for maintaining the integrity of surveillance systems.
- Analysts will provide a status update on the onboarding process, ensuring transparency and accountability.

**2.6 Persistency Table Detection and Reconciliation**

- The application will detect persistency tables used by production surveillance models.
- A reconciliation process will compare these tables with the detected data feeds to identify mismatches and potential gaps.
- Any discrepancies will be flagged for further investigation and resolution, ensuring data consistency and completeness.

#### 3. Non-Functional Requirements

**3.1 Performance**

- The application should detect new data feeds and send notifications within 24 hours of data load completion.
- The dashboard should load within 2 seconds for all users, ensuring a smooth user experience.
- The reconciliation process should complete within a reasonable timeframe (e.g., within 1 hour of initiation) to maintain efficiency.

**3.2 Security**

- Only authorized personnel (database administrators and surveillance business analysts) should have access to the respective sections of the dashboard.
- Data integrity and confidentiality must be maintained throughout the process, protecting sensitive information.

**3.3 Usability**

- The user interface should be intuitive and easy to navigate, minimizing the learning curve for users.
- Clear instructions and error messages should be provided to guide users through the process, enhancing user experience.

**3.4 Reliability**

- The system should have 99.9% uptime, ensuring continuous availability.
- Robust error handling and logging mechanisms should be in place to track any issues, aiding in prompt resolution.

#### 4. System Architecture

**4.1 Components**

- **Data Feed Detection Module**: Scans the database and detects new data feeds.
- **Notification Module**: Sends email notifications to relevant stakeholders.
- **Administrator Dashboard**: Interface for database administrators to provide details and approvals.
- **Analyst Dashboard**: Interface for surveillance business analysts to review and approve feeds.
- **Reconciliation Module**: Compares persistency tables with detected data feeds and identifies discrepancies.

**4.2 Integration**

- The application will integrate with the existing database to access load logs and data feeds.
- Email notifications will be sent via the corporate email system.
- The dashboard will be part of the existing web application, ensuring a seamless user experience.

#### 5. User Interface

**5.1 Administrator Dashboard**

- Sections for entering feed details, ensuring comprehensive data capture.
- Approval buttons for regulatory relevance, simplifying the approval process.

**5.2 Analyst Dashboard**

- Sections for reviewing feed information, facilitating thorough analysis.
- Status update fields, ensuring clear communication and tracking.

**5.3 Reconciliation Dashboard**

- Display detected persistency tables and comparison results.
- Highlight mismatches and gaps for investigation, aiding in quick resolution.

#### 6. Workflow Diagram

**Workflow Representation**

1. **Data Feed Detection**:
   - The system scans the database daily.
   - New tables with successfully loaded data are detected.

2. **Notification and Workflow Initiation**:
   - Email notifications are sent to database administrators.
   - Administrators access the dashboard to provide feed details.

3. **Administrator Input**:
   - Administrators fill in details such as loader name, frequency, data type, etc.
   - Approval for regulatory relevance is provided.

4. **Notification to Analysts**:
   - Email notifications are sent to surveillance business analysts.
   - Analysts access the dashboard to review new feeds.

5. **Analyst Review**:
   - Analysts review feed information and determine its relevance for surveillance models.
   - Analysts update the status of the feed onboarding process.

6. **Persistency Table Detection**:
   - The system detects persistency tables used by production surveillance models.

7. **Reconciliation Process**:
   - Detected persistency tables are compared with data feeds.
   - Mismatches and gaps are identified and flagged for investigation.

---

**Signatures**

*Prepared by:*
[Your Name]

*Reviewed by:*
[Reviewer Name]

*Approved by:*
[Approver Name]

---

This FSD outlines the functional requirements and specifications for the Data Feed Detection, Onboarding, and Reconciliation Application. This ensures all new data feeds are properly detected, reviewed, reconciled, and utilized for compliance purposes, enhancing data governance and regulatory adherence.










# Functional Specification Document (FSD)

## 1. Introduction

### 1.1 Purpose
This document details the functional specifications for an application designed to trace data filtered during pre-processing before surveillance models run. The application includes a configurable dashboard for data consultation and lineage visualization. It aims to provide transparency and traceability by identifying filtered messages and displaying their flow from source to alert generation.

### 1.2 Scope
The application allows users to configure data sources, define filtering rules, and compare data to detect and justify filtered messages. It also provides a data lineage view to track data flow from the data source to the alert generation phase. This tool is intended for database administrators and surveillance business analysts who need to understand data filtering processes and ensure data integrity and compliance.

### 1.3 Definitions, Acronyms, and Abbreviations
- **FSD:** Functional Specification Document
- **ID:** Identifier
- **UI:** User Interface
- **Datamart:** A subset of a data warehouse focused on a particular area of interest.

### 1.4 References
- [Surveillance Model Documentation]
- [Database Schema]
- [User Guide]

## 2. Overall Description

### 2.1 Product Perspective
The application integrates into the existing surveillance system, enhancing data transparency and traceability by identifying filtered messages and providing detailed lineage from data sources to alert generation. It serves as a crucial tool for monitoring and ensuring the accuracy and reliability of data used in surveillance models.

### 2.2 Product Functions
- Configure data sources and target tables.
- Define and apply filtering rules.
- Compare data to detect filtered messages.
- Display counts of filtered rows.
- Provide technical and functional justifications for filtered messages.
- Visualize data lineage from source to alert generation.

### 2.3 User Characteristics
The primary users are database administrators and surveillance business analysts who require insights into data filtering and flow within the surveillance system. These users typically have a technical background and are responsible for maintaining data integrity, compliance, and performance of surveillance models.

### 2.4 Assumptions and Dependencies
- The application assumes access to the required databases and tables.
- Dependencies include the underlying database management system and the existing surveillance models.
- The system should be compatible with the data formats and schemas used in the current surveillance infrastructure.

## 3. Functional Requirements

### 3.1 Data Source Configuration

#### 3.1.1 Description
Users can configure the source of data, including the selection of the persistency table and the target datamart table name. This configuration allows the application to retrieve and process the relevant data for analysis.

#### 3.1.2 Inputs
- Source table name
- Persistency table name
- Target datamart table name

#### 3.1.3 Processing
The application stores the configuration settings and uses them to access the necessary tables. It retrieves data from the source table and persistency table, then prepares it for comparison against the target datamart table.

#### 3.1.4 Outputs
- Confirmation message on successful configuration
- Error messages for invalid inputs
- Log entries detailing the configuration process

### 3.2 Data Filtering and Comparison

#### 3.2.1 Description
The application traces and compares data to detect filtered messages by utilizing a message ID column. It identifies discrepancies between the source and target tables, highlighting messages that have been filtered out during pre-processing.

#### 3.2.2 Inputs
- Message ID column
- Filtering rules

#### 3.2.3 Processing
The application performs a comparison of data between the source, persistency, and target tables based on the message ID column. It identifies filtered messages and counts the number of rows filtered out during the pre-processing stage.

#### 3.2.4 Outputs
- Count of filtered rows
- Detailed logs of filtered messages, including their IDs and reasons for filtering
- Reports summarizing the filtering results

### 3.3 Filtering Rules

#### 3.3.1 Description
Users can define filtering rules to justify filtered messages both technically and functionally. These rules determine which messages are filtered out during pre-processing based on specific conditions.

#### 3.3.2 Inputs
- Rule name
- Condition (e.g., message type, value thresholds)

#### 3.3.3 Processing
The application applies the defined rules to filter data during the pre-processing stage. It evaluates each message against the filtering conditions and marks those that meet the criteria as filtered.

#### 3.3.4 Outputs
- Report or log of applied filtering rules and their results
- Justification for each filtered message based on the applied rules

### 3.4 Dashboard and Data Lineage

#### 3.4.1 Description
The dashboard provides functionalities including a data lineage section that visualizes data flow from the source to alert generation. Users can consult the dashboard to see the entire lifecycle of data, from initial ingestion to final alert generation.

#### 3.4.2 Inputs
- Data source configuration
- Filtering results

#### 3.4.3 Processing
The application compiles and displays data lineage information and filtering results on the dashboard. It aggregates data from different stages of the process to provide a comprehensive view of data flow and filtering activities.

#### 3.4.4 Outputs
- Visual representations of data flow
- Counts of alerts generated
- Interactive charts and graphs showing data lineage and filtering metrics

## 4. System Features

### 4.1 User Interface
The UI includes sections for data source configuration, filtering rules definition, data comparison results, and data lineage visualization. It is designed to be intuitive and user-friendly, allowing users to easily navigate and access the functionalities they need.

### 4.2 Reports
The application generates detailed reports that include filtered messages, applied rules, and data lineage. These reports can be used for auditing, compliance, and performance analysis.

### 4.3 Notifications
The application provides notifications for configuration changes, filtering results, and system errors. These notifications help users stay informed about important events and potential issues.

### 4.4 Data Export
Users can export data and reports in formats such as CSV and PDF. This feature allows users to share information with stakeholders and perform further analysis outside the application.

## 5. Non-Functional Requirements

### 5.1 Performance Requirements
- The application should respond to user actions within 2 seconds.
- Data comparison and filtering processes should complete within 1 minute for typical datasets.
- The system should handle large volumes of data efficiently, ensuring timely processing and analysis.

### 5.2 Security Requirements
- User authentication and authorization are required for access.
- Data should be encrypted in transit and at rest.
- The application should comply with relevant data protection regulations and industry standards.

### 5.3 Usability Requirements
- The application should have an intuitive and user-friendly interface.
- Accessible for users with varying levels of technical expertise.
- The design should prioritize clarity, ease of use, and efficient navigation.

### 5.4 Reliability Requirements
- The application should have an uptime of 99.9%.
- Error handling should ensure graceful degradation and informative error messages.
- The system should provide robust logging and monitoring to facilitate troubleshooting and maintenance.

### 5.5 Maintainability Requirements
- The application should support easy updates and configuration changes.
- Documentation should be provided for maintenance and support.
- The codebase should be modular and well-documented to facilitate future enhancements and debugging.

## 6. Appendices

### 6.1 Glossary
- **Filtered Message:** A message that has been excluded based on predefined rules.
- **Data Lineage:** The historical and chronological flow of data from source to destination.
- **Persistency Table:** A table that stores intermediate data during processing.
- **Datamart:** A subset of a data warehouse focused on a particular area of interest.

### 6.2 Diagrams
- **Data Flow Diagram:** (Include diagram showing data flow from source to alerts generation)
- **Architecture Diagram:** (Include diagram showing the system architecture)

### 6.3 Sample Data
- **Input Data Example:** (Provide sample input data)
- **Output Data Example:** (Provide sample output data)

### 6.4 References
- [Surveillance Model Documentation]
- [Database Schema]
- [User Guide]

---

This revised FSD includes more detailed explanations of each process, providing a comprehensive overview of the application's functionalities and requirements.





